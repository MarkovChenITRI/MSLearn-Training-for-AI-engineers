{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e641d76-4aa7-4380-b0c3-0cea011053e2",
   "metadata": {},
   "source": [
    "# Get started with Azure AI Services for Private Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4998a-4c94-4583-bbb0-eb267bdaed43",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "`Azure AI services` provide **Docker containers** that let you can keep the data closer to your host. To deploy and use an Azure AI services container, the following three activities must occur:\n",
    "\n",
    "1. The container image for the specific Azure AI services API you want to use is downloaded and deployed to a container host, such as a local Docker server, an **Azure Container Instance (ACI)**, or **Azure Kubernetes Service (AKS)**.\n",
    "2. Client applications **submit data to the endpoint** provided by the containerized service, and retrieve results just as they would from an Azure AI services cloud resource in Azure.\n",
    "3. Periodically, **usage metrics** for the containerized service are sent to an Azure AI services resource in Azure in order to calculate billing for the service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27513968-f65e-4549-acb6-099e801bb0cf",
   "metadata": {},
   "source": [
    "![image.png](./assets/ai-services-container.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00ff7521-35cc-4982-9dfb-34f56dd46923",
   "metadata": {},
   "source": [
    "this architecture gives the features and benefits below:\n",
    "* **Immutable infrastructure**:Enable DevOps teams to leverage a consistent and reliable set of known system functions. \n",
    "* **Control over data**: Choose where your data gets processed by Azure AI services\n",
    "* **Control over model updates**: Flexibility in versioning and updating of models deployed in their solutions\n",
    "* **Portable architecture**: Enables the creation of a portable application architecture that can be deployed on Azure, on-premises and the edge.\n",
    "* **High throughput & low latency**: Enabling Azure AI services to run physically close to their application logic and data.\n",
    "* **Scalability**: With the ever growing popularity of containerization and container orchestration software, such as Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748878ea-83e0-4d5b-8ff9-43a4f34dd4cd",
   "metadata": {},
   "source": [
    "### Build Your own Cognitive Service\n",
    "\n",
    "Here we use **Document Intelligence** as example to demonstrate how to install and run a container.<br> \n",
    "\n",
    "#### **Prerequisites**\n",
    "* An active **Azure Account**\n",
    "* **Azure Key Vault** for secure API Keys (*Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42bb87a-404b-49b3-aaa6-a2abf2fe1b49",
   "metadata": {},
   "source": [
    "#### Step1. Create your own **Document Intelligence Service** and get the `API Key` and `Endpoint`.\n",
    "<img src=\"./assets/azure-ai-service-portal.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28eb04f-c969-4f48-ba64-0c553a090c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORM_RECOGNIZER_ENDPOINT_URI = 'https://ces-document-intelligence.cognitiveservices.azure.com/'\n",
    "FORM_RECOGNIZER_KEY = '30bfa09ff3fe426c83e434a921277770'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008db6c-4e87-4834-b541-c679a12e268a",
   "metadata": {},
   "source": [
    "#### Step2. Install Azure AI Python SDK for **Document Intelligence**.\n",
    "\n",
    "> you can browse and choice other cognitive services from [HERE](https://learn.microsoft.com/en-us/python/api/overview/azure/cognitive-services?view=azure-python-preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3280ec11-59ee-4b6c-ab74-861b64e20d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: azure-ai-documentintelligence==1.0.0b4 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.0.0b4)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from azure-ai-documentintelligence==1.0.0b4) (0.6.1)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from azure-ai-documentintelligence==1.0.0b4) (1.31.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from azure-ai-documentintelligence==1.0.0b4) (4.9.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from azure-core>=1.30.0->azure-ai-documentintelligence==1.0.0b4) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from azure-core>=1.30.0->azure-ai-documentintelligence==1.0.0b4) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence==1.0.0b4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence==1.0.0b4) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence==1.0.0b4) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence==1.0.0b4) (2023.11.17)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tabulate in c:\\users\\a0973\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-documentintelligence==1.0.0b4\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6048aa-5bdf-4a9f-b62c-dd16596facc6",
   "metadata": {},
   "source": [
    "#### Step3. Create an Analysis Client with `prebuilt-layout` Module\n",
    "\n",
    "* `model_id`: Required, d-type(str). Use this to specify the **Custom model ID** or **Prebuilt model ID** for your analysis process. **Prebuilt model IDs** supported can be found here: https://aka.ms/azsdk/formrecognizer/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23aceaca-7892-4ad8-8945-47527bdea15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "\n",
    "credential = AzureKeyCredential(FORM_RECOGNIZER_KEY)\n",
    "client = DocumentIntelligenceClient(endpoint=FORM_RECOGNIZER_ENDPOINT_URI, credential=credential)\n",
    "\n",
    "with open('./assets/attention-is-all-you-need.pdf', \"rb\") as f:\n",
    "    poller = client.begin_analyze_document(model_id=\"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\")\n",
    "    result: AnalyzeResult = poller.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac196669-977b-4258-ab1f-193bab4d3870",
   "metadata": {},
   "source": [
    "#### Step4. Perform a Proof-of-Concept of its Functionality on Cloud\n",
    "> Check more usage of **Azure AI Python SDK** with [Offical Technical Report](https://learn.microsoft.com/zh-tw/python/api/azure-ai-documentintelligence/azure.ai.documentintelligence.documentintelligenceclient?view=azure-python-preview).\n",
    "\n",
    "<img src=\"./assets/document-layout-example.png\" width=\"480\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ffe97-aef9-47c3-944c-2d6cc0099032",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### a. HandWritten Contect Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f7a7cb-f68d-4829-bc40-06dacdf30c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document contains handwritten content\n"
     ]
    }
   ],
   "source": [
    "if result.styles and any([style.is_handwritten for style in result.styles]):\n",
    "    print(\"Document contains handwritten content\")\n",
    "else:\n",
    "    print(\"Document does not contain handwritten content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43635899-123e-4bb7-89c7-9d3cc2d02dfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### b. Extracting contents of each pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ceaf40-4890-4442-8fcb-b81a102213c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page #1 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: arXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n",
      " - 1: Provided proper attribution is provided, Google hereby grants permission to\n",
      " - 2: reproduce the tables and figures in this paper solely for use in journalistic or\n",
      " - 3: scholarly works.\n",
      " - 4: Attention Is All You Need\n",
      " - 5: Ashish Vaswani*\n",
      " - 6: Google Brain\n",
      " - 7: Noam Shazeer*\n",
      " - 8: Google Brain\n",
      " - 9: Niki Parmar*\n",
      " - 10: Google Research\n",
      " - 11: Jakob Uszkoreit*\n",
      " - 12: Google Research\n",
      " - 13: avaswani@google.com\n",
      " - 14: noam@google.com\n",
      " - 15: nikip@google.com\n",
      " - 16: usz@google.com\n",
      " - 17: Llion Jones*\n",
      " - 18: Google Research\n",
      " - 19: Aidan N. Gomez* +\n",
      " - 20: University of Toronto\n",
      " - 21: Łukasz Kaiser*\n",
      " - 22: Google Brain\n",
      " - 23: llion@google.com\n",
      " - 24: aidan@cs.toronto.edu\n",
      " - 25: lukaszkaiser@google.com\n",
      " - 26: Illia Polosukhin* *\n",
      " - 27: illia.polosukhin@gmail.com\n",
      " - 28: Abstract\n",
      " - 29: The dominant sequence transduction models are based on complex recurrent or\n",
      " - 30: convolutional neural networks that include an encoder and a decoder. The best\n",
      " - 31: performing models also connect the encoder and decoder through an attention\n",
      " - 32: mechanism. We propose a new simple network architecture, the Transformer,\n",
      " - 33: based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      " - 34: entirely. Experiments on two machine translation tasks show these models to\n",
      " - 35: be superior in quality while being more parallelizable and requiring significantly\n",
      " - 36: less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      " - 37: to-German translation task, improving over the existing best results, including\n",
      " - 38: ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      " - 39: our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      " - 40: training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      " - 41: best models from the literature. We show that the Transformer generalizes well to\n",
      " - 42: other tasks by applying it successfully to English constituency parsing both with\n",
      " - 43: large and limited training data.\n",
      " - 44: *Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      " - 45: the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      " - 46: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      " - 47: attention and the parameter-free position representation and became the other person involved in nearly every\n",
      " - 48: detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      " - 49: tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      " - 50: efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      " - 51: implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      " - 52: our research.\n",
      " - 53: + Work performed while at Google Brain.\n",
      " - 54: + Work performed while at Google Research.\n",
      " - 55: 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "Page #2 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: 1 Introduction\n",
      " - 1: Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      " - 2: in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      " - 3: transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
      " - 4: efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      " - 5: architectures [38, 24, 15].\n",
      " - 6: Recurrent models typically factor computation along the symbol positions of the input and output\n",
      " - 7: sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      " - 8: states ht, as a function of the previous hidden state ht-1 and the input for position t. This inherently\n",
      " - 9: sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      " - 10: sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      " - 11: significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      " - 12: computation [32], while also improving model performance in case of the latter. The fundamental\n",
      " - 13: constraint of sequential computation, however, remains.\n",
      " - 14: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      " - 15: tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      " - 16: the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      " - 17: are used in conjunction with a recurrent network.\n",
      " - 18: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      " - 19: relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      " - 20: The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      " - 21: translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      " - 22: 2 Background\n",
      " - 23: The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      " - 24: [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      " - 25: block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      " - 26: the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      " - 27: in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      " - 28: it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      " - 29: reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      " - 30: to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      " - 31: described in section 3.2.\n",
      " - 32: Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      " - 33: of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      " - 34: used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      " - 35: textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      " - 36: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      " - 37: aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      " - 38: language modeling tasks [34].\n",
      " - 39: To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      " - 40: entirely on self-attention to compute representations of its input and output without using sequence-\n",
      " - 41: aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      " - 42: self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      " - 43: 3 Model Architecture\n",
      " - 44: Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      " - 45: Here, the encoder maps an input sequence of symbol representations (x1, ... , In) to a sequence\n",
      " - 46: of continuous representations z = (Z1, ... , Zn). Given z, the decoder then generates an output\n",
      " - 47: sequence (y1, ... , ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      " - 48: [10], consuming the previously generated symbols as additional input when generating the next.\n",
      " - 49: 2\n",
      "Page #3 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: Output\n",
      " - 1: Probabilities\n",
      " - 2: Softmax\n",
      " - 3: Linear\n",
      " - 4: Add & Norm\n",
      " - 5: Feed\n",
      " - 6: Forward\n",
      " - 7: Add & Norm\n",
      " - 8: Add & Norm\n",
      " - 9: Feed\n",
      " - 10: Forward\n",
      " - 11: Multi-Head\n",
      " - 12: Attention\n",
      " - 13: Nx\n",
      " - 14: Nx\n",
      " - 15: Add & Norm\n",
      " - 16: Add & Norm\n",
      " - 17: Multi-Head\n",
      " - 18: Attention\n",
      " - 19: Masked\n",
      " - 20: Multi-Head\n",
      " - 21: Attention\n",
      " - 22: Positional\n",
      " - 23: Encoding\n",
      " - 24: Positional\n",
      " - 25: Encoding\n",
      " - 26: Input\n",
      " - 27: Embedding\n",
      " - 28: Output\n",
      " - 29: Embedding\n",
      " - 30: Inputs\n",
      " - 31: Outputs\n",
      " - 32: (shifted right)\n",
      " - 33: Figure 1: The Transformer - model architecture.\n",
      " - 34: The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      " - 35: connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      " - 36: respectively.\n",
      " - 37: 3.1 Encoder and Decoder Stacks\n",
      " - 38: Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      " - 39: sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      " - 40: wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      " - 41: the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
      " - 42: LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      " - 43: itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      " - 44: layers, produce outputs of dimension dmodel = 512.\n",
      " - 45: Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      " - 46: sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      " - 47: attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      " - 48: around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      " - 49: sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      " - 50: masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      " - 51: predictions for position i can depend only on the known outputs at positions less than i.\n",
      " - 52: 3.2 Attention\n",
      " - 53: An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      " - 54: where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      " - 55: 3\n",
      "Page #4 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: Scaled Dot-Product Attention\n",
      " - 1: 1\n",
      " - 2: MatMul\n",
      " - 3: 1\n",
      " - 4: SoftMax\n",
      " - 5: 1\n",
      " - 6: Mask (opt.)\n",
      " - 7: 1\n",
      " - 8: Scale\n",
      " - 9: 1\n",
      " - 10: MatMul\n",
      " - 11: 1\n",
      " - 12: 1\n",
      " - 13: Q\n",
      " - 14: K\n",
      " - 15: V\n",
      " - 16: Multi-Head Attention\n",
      " - 17: Linear\n",
      " - 18: Concat\n",
      " - 19: Scaled Dot-Product\n",
      " - 20: Attention\n",
      " - 21: h\n",
      " - 22: Linear\n",
      " - 23: Linear\n",
      " - 24: Linear\n",
      " - 25: V\n",
      " - 26: K\n",
      " - 27: Q\n",
      " - 28: Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      " - 29: attention layers running in parallel.\n",
      " - 30: of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      " - 31: query with the corresponding key.\n",
      " - 32: 3.2.1 Scaled Dot-Product Attention\n",
      " - 33: We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      " - 34: queries and keys of dimension dk, and values of dimension d„. We compute the dot products of the\n",
      " - 35: query with all keys, divide each by vdk, and apply a softmax function to obtain the weights on the\n",
      " - 36: values.\n",
      " - 37: In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      " - 38: into a matrix Q. The keys and values are also packed together into matrices K and V. We compute\n",
      " - 39: the matrix of outputs as:\n",
      " - 40: Attention(Q, K,V) = softmax(\n",
      " - 41: Vdk\n",
      " - 42: QKT\n",
      " - 43: V\n",
      " - 44: (1)\n",
      " - 45: The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      " - 46: plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      " - 47: of\n",
      " - 48: - . Additive attention computes the compatibility function using a feed-forward network with\n",
      " - 49: Vdk\n",
      " - 50: a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      " - 51: much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      " - 52: matrix multiplication code.\n",
      " - 53: While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      " - 54: dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      " - 55: dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      " - 56: extremely small gradients 4. To counteract this effect, we scale the dot products by a.\n",
      " - 57: 3.2.2 Multi-Head Attention\n",
      " - 58: Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      " - 59: we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
      " - 60: linear projections to dk, dk and d„ dimensions, respectively. On each of these projected versions of\n",
      " - 61: queries, keys and values we then perform the attention function in parallel, yielding dy-dimensional\n",
      " - 62: 4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      " - 63: variables with mean 0 and variance 1. Then their dot product, q · k = Sik1 qiki, has mean 0 and variance dk-\n",
      " - 64: 4\n",
      "Page #5 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: output values. These are concatenated and once again projected, resulting in the final values, as\n",
      " - 1: depicted in Figure 2.\n",
      " - 2: Multi-head attention allows the model to jointly attend to information from different representation\n",
      " - 3: subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      " - 4: MultiHead(Q,K,V) = Concat(head1, ... , headh)WO\n",
      " - 5: where headi = Attention(QW,2,KWK,VW;)\n",
      " - 6: Where the projections are parameter matrices W,2 € Rdmodel X dk , W K E Rdmodel X dk , WV E Rdmodel X du\n",
      " - 7: and WO E Rhdy X dmodel\n",
      " - 8: In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
      " - 9: dk = du = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
      " - 10: is similar to that of single-head attention with full dimensionality.\n",
      " - 11: 3.2.3 Applications of Attention in our Model\n",
      " - 12: The Transformer uses multi-head attention in three different ways:\n",
      " - 13: · In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      " - 14: and the memory keys and values come from the output of the encoder. This allows every\n",
      " - 15: position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      " - 16: typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      " - 17: [38, 2, 9].\n",
      " - 18: · The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      " - 19: and queries come from the same place, in this case, the output of the previous layer in the\n",
      " - 20: encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      " - 21: encoder.\n",
      " - 22: · Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      " - 23: all positions in the decoder up to and including that position. We need to prevent leftward\n",
      " - 24: information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      " - 25: inside of scaled dot-product attention by masking out (setting to -co) all values in the input\n",
      " - 26: of the softmax which correspond to illegal connections. See Figure 2.\n",
      " - 27: 3.3 Position-wise Feed-Forward Networks\n",
      " - 28: In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      " - 29: connected feed-forward network, which is applied to each position separately and identically. This\n",
      " - 30: consists of two linear transformations with a ReLU activation in between.\n",
      " - 31: FFN(x) =max(0,xW1+b1)W2+b2\n",
      " - 32: (2)\n",
      " - 33: While the linear transformations are the same across different positions, they use different parameters\n",
      " - 34: from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      " - 35: The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      " - 36: dff = 2048.\n",
      " - 37: 3.4 Embeddings and Softmax\n",
      " - 38: Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      " - 39: tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      " - 40: mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      " - 41: our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      " - 42: linear transformation, similar to [30]. In the embedding layers, we multiply those weights by Vdmodel.\n",
      " - 43: 5\n",
      "Page #6 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      " - 1: for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
      " - 2: size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      " - 3: Layer Type\n",
      " - 4: Complexity per Layer\n",
      " - 5: Sequential\n",
      " - 6: Operations\n",
      " - 7: Maximum Path Length\n",
      " - 8: Self-Attention\n",
      " - 9: O(n2 .d)\n",
      " - 10: O(1)\n",
      " - 11: O(1)\n",
      " - 12: Recurrent\n",
      " - 13: O(n ·d2)\n",
      " - 14: O(n)\n",
      " - 15: O(n)\n",
      " - 16: Convolutional\n",
      " - 17: O(k ·n·d2)\n",
      " - 18: O(1)\n",
      " - 19: O(logk(n))\n",
      " - 20: Self-Attention (restricted)\n",
      " - 21: O(r . n . d)\n",
      " - 22: O(1)\n",
      " - 23: O(n/r)\n",
      " - 24: 3.5 Positional Encoding\n",
      " - 25: Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      " - 26: order of the sequence, we must inject some information about the relative or absolute position of the\n",
      " - 27: tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      " - 28: bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      " - 29: as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      " - 30: learned and fixed [9].\n",
      " - 31: In this work, we use sine and cosine functions of different frequencies:\n",
      " - 32: PE(pos,2i) = sin(pos/100002i/dmodel )\n",
      " - 33: PE(pos,2i+1) = cos(pos/100002i/dmodel )\n",
      " - 34: where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
      " - 35: corresponds to a sinusoid. The wavelengths form a geometric progression from 27 to 10000 . 27. We\n",
      " - 36: chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      " - 37: relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\n",
      " - 38: PEpos.\n",
      " - 39: We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
      " - 40: versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      " - 41: because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      " - 42: during training.\n",
      " - 43: 4 Why Self-Attention\n",
      " - 44: In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      " - 45: tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      " - 46: (x1, ... , In) to another sequence of equal length (21, ... , Zn), with xi, Zi E Rd, such as a hidden\n",
      " - 47: layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      " - 48: consider three desiderata.\n",
      " - 49: One is the total computational complexity per layer. Another is the amount of computation that can\n",
      " - 50: be parallelized, as measured by the minimum number of sequential operations required.\n",
      " - 51: The third is the path length between long-range dependencies in the network. Learning long-range\n",
      " - 52: dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      " - 53: ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      " - 54: traverse in the network. The shorter these paths between any combination of positions in the input\n",
      " - 55: and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
      " - 56: the maximum path length between any two input and output positions in networks composed of the\n",
      " - 57: different layer types.\n",
      " - 58: As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      " - 59: executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      " - 60: computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      " - 61: 6\n",
      "Page #7 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: length n is smaller than the representation dimensionality d, which is most often the case with\n",
      " - 1: sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      " - 2: [38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
      " - 3: very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
      " - 4: the input sequence centered around the respective output position. This would increase the maximum\n",
      " - 5: path length to O(n/r). We plan to investigate this approach further in future work.\n",
      " - 6: A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      " - 7: positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
      " - 8: or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\n",
      " - 9: between any two positions in the network. Convolutional layers are generally more expensive than\n",
      " - 10: recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n",
      " - 11: considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
      " - 12: convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      " - 13: the approach we take in our model.\n",
      " - 14: As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      " - 15: from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      " - 16: heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      " - 17: and semantic structure of the sentences.\n",
      " - 18: 5 Training\n",
      " - 19: This section describes the training regime for our models.\n",
      " - 20: 5.1 Training Data and Batching\n",
      " - 21: We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      " - 22: sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\n",
      " - 23: target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      " - 24: 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      " - 25: vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      " - 26: batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      " - 27: target tokens.\n",
      " - 28: 5.2 Hardware and Schedule\n",
      " - 29: We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      " - 30: the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      " - 31: trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      " - 32: bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      " - 33: (3.5 days).\n",
      " - 34: 5.3 Optimizer\n",
      " - 35: We used the Adam optimizer [20] with ß1 =0.9, ß2= 0.98 and € = 10-9. We varied the learning\n",
      " - 36: rate over the course of training, according to the formula:\n",
      " - 37: lrate = dmodel . min(step_num-0.5, step_num . warmup_steps-1.5)\n",
      " - 38: (3)\n",
      " - 39: This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
      " - 40: and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      " - 41: warmup_steps = 4000.\n",
      " - 42: 5.4 Regularization\n",
      " - 43: We employ three types of regularization during training:\n",
      " - 44: 7\n",
      "Page #8 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      " - 1: English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      " - 2: Model\n",
      " - 3: BLEU\n",
      " - 4: Training Cost (FLOPs)\n",
      " - 5: EN-DE\n",
      " - 6: EN-FR\n",
      " - 7: EN-DE\n",
      " - 8: EN-FR\n",
      " - 9: ByteNet [18]\n",
      " - 10: 23.75\n",
      " - 11: Deep-Att + PosUnk [39]\n",
      " - 12: 39.2\n",
      " - 13: 1.0 · 1020\n",
      " - 14: GNMT + RL [38]\n",
      " - 15: 24.6\n",
      " - 16: 39.92\n",
      " - 17: 2.3 . 1019\n",
      " - 18: 1.4 . 1020\n",
      " - 19: ConvS2S [9]\n",
      " - 20: 25.16\n",
      " - 21: 40.46\n",
      " - 22: 9.6 . 1018\n",
      " - 23: 1.5 . 1020\n",
      " - 24: MoE [32]\n",
      " - 25: 26.03\n",
      " - 26: 40.56\n",
      " - 27: 2.0 · 1019\n",
      " - 28: 1.2 . 1020\n",
      " - 29: Deep-Att + PosUnk Ensemble [39]\n",
      " - 30: 40.4\n",
      " - 31: 8.0 · 1020\n",
      " - 32: GNMT + RL Ensemble [38]\n",
      " - 33: 26.30\n",
      " - 34: 41.16\n",
      " - 35: 1.8 . 1020\n",
      " - 36: 1.1 . 1021\n",
      " - 37: ConvS2S Ensemble [9]\n",
      " - 38: 26.36\n",
      " - 39: 41.29\n",
      " - 40: 7.7 .1019\n",
      " - 41: 1.2 . 1021\n",
      " - 42: Transformer (base model)\n",
      " - 43: 27.3\n",
      " - 44: 38.1\n",
      " - 45: 3.3 . 1018\n",
      " - 46: Transformer (big)\n",
      " - 47: 28.4\n",
      " - 48: 41.8\n",
      " - 49: 2.3 · 1019\n",
      " - 50: Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\n",
      " - 51: sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      " - 52: positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      " - 53: Pdrop = 0.1.\n",
      " - 54: Label Smoothing During training, we employed label smoothing of value €ls = 0.1 [36]. This\n",
      " - 55: hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      " - 56: 6 Results\n",
      " - 57: 6.1 Machine Translation\n",
      " - 58: On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      " - 59: in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      " - 60: BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      " - 61: listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
      " - 62: surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      " - 63: the competitive models.\n",
      " - 64: On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      " - 65: outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
      " - 66: previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      " - 67: dropout rate Pdrop = 0.1, instead of 0.3.\n",
      " - 68: For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      " - 69: were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      " - 70: used beam search with a beam size of 4 and length penalty Q = 0.6 [38]. These hyperparameters\n",
      " - 71: were chosen after experimentation on the development set. We set the maximum output length during\n",
      " - 72: inference to input length + 50, but terminate early when possible [38].\n",
      " - 73: Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      " - 74: architectures from the literature. We estimate the number of floating point operations used to train a\n",
      " - 75: model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      " - 76: single-precision floating-point capacity of each GPU 5.\n",
      " - 77: 6.2 Model Variations\n",
      " - 78: To evaluate the importance of different components of the Transformer, we varied our base model\n",
      " - 79: in different ways, measuring the change in performance on English-to-German translation on the\n",
      " - 80: 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      " - 81: 8\n",
      "Page #9 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      " - 1: model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      " - 2: perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      " - 3: per-word perplexities.\n",
      " - 4: N\n",
      " - 5: dmodel\n",
      " - 6: dff\n",
      " - 7: h\n",
      " - 8: dk\n",
      " - 9: dv\n",
      " - 10: Pdrop\n",
      " - 11: Els\n",
      " - 12: train\n",
      " - 13: steps\n",
      " - 14: PPL\n",
      " - 15: (dev)\n",
      " - 16: BLEU\n",
      " - 17: (dev)\n",
      " - 18: params\n",
      " - 19: × 106\n",
      " - 20: base\n",
      " - 21: 6\n",
      " - 22: 512\n",
      " - 23: 2048\n",
      " - 24: 8\n",
      " - 25: 64\n",
      " - 26: 64\n",
      " - 27: 0.1\n",
      " - 28: 0.1\n",
      " - 29: 100K\n",
      " - 30: 4.92\n",
      " - 31: 25.8\n",
      " - 32: 65\n",
      " - 33: (A)\n",
      " - 34: 1\n",
      " - 35: 512\n",
      " - 36: 512\n",
      " - 37: 5.29\n",
      " - 38: 24.9\n",
      " - 39: 4\n",
      " - 40: 128\n",
      " - 41: 128\n",
      " - 42: 5.00\n",
      " - 43: 25.5\n",
      " - 44: 16\n",
      " - 45: 32\n",
      " - 46: 32\n",
      " - 47: 4.91\n",
      " - 48: 25.8\n",
      " - 49: 32\n",
      " - 50: 16\n",
      " - 51: 16\n",
      " - 52: 5.01\n",
      " - 53: 25.4\n",
      " - 54: (B)\n",
      " - 55: 16\n",
      " - 56: 5.16\n",
      " - 57: 25.1\n",
      " - 58: 58\n",
      " - 59: 32\n",
      " - 60: 5.01\n",
      " - 61: 25.4\n",
      " - 62: 60\n",
      " - 63: (C)\n",
      " - 64: 2\n",
      " - 65: 6.11\n",
      " - 66: 23.7\n",
      " - 67: 36\n",
      " - 68: 4\n",
      " - 69: 5.19\n",
      " - 70: 25.3\n",
      " - 71: 50\n",
      " - 72: 8\n",
      " - 73: 4.88\n",
      " - 74: 25.5\n",
      " - 75: 80\n",
      " - 76: 256\n",
      " - 77: 32\n",
      " - 78: 32\n",
      " - 79: 5.75\n",
      " - 80: 24.5\n",
      " - 81: 28\n",
      " - 82: 1024\n",
      " - 83: 128\n",
      " - 84: 128\n",
      " - 85: 4.66\n",
      " - 86: 26.0\n",
      " - 87: 168\n",
      " - 88: 1024\n",
      " - 89: 5.12\n",
      " - 90: 25.4\n",
      " - 91: 53\n",
      " - 92: 4096\n",
      " - 93: 4.75\n",
      " - 94: 26.2\n",
      " - 95: 90\n",
      " - 96: (D)\n",
      " - 97: 0.0\n",
      " - 98: 5.77\n",
      " - 99: 24.6\n",
      " - 100: 0.2\n",
      " - 101: 4.95\n",
      " - 102: 25.5\n",
      " - 103: 0.0\n",
      " - 104: 4.67\n",
      " - 105: 25.3\n",
      " - 106: 0.2\n",
      " - 107: 5.47\n",
      " - 108: 25.7\n",
      " - 109: (E)\n",
      " - 110: positional embedding instead of sinusoids\n",
      " - 111: 4.92\n",
      " - 112: 25.7\n",
      " - 113: big\n",
      " - 114: 6\n",
      " - 115: 1024\n",
      " - 116: 4096\n",
      " - 117: 16\n",
      " - 118: 0.3\n",
      " - 119: 300K\n",
      " - 120: 4.33\n",
      " - 121: 26.4\n",
      " - 122: 213\n",
      " - 123: development set, newstest2013. We used beam search as described in the previous section, but no\n",
      " - 124: checkpoint averaging. We present these results in Table 3.\n",
      " - 125: In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      " - 126: keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      " - 127: attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      " - 128: In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      " - 129: suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      " - 130: function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      " - 131: bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      " - 132: sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
      " - 133: results to the base model.\n",
      " - 134: 6.3 English Constituency Parsing\n",
      " - 135: To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      " - 136: constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      " - 137: constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      " - 138: models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      " - 139: We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      " - 140: Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      " - 141: using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      " - 142: [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      " - 143: for the semi-supervised setting.\n",
      " - 144: We performed only a small number of experiments to select the dropout, both attention and residual\n",
      " - 145: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      " - 146: remained unchanged from the English-to-German base translation model. During inference, we\n",
      " - 147: 9\n",
      "Page #10 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      " - 1: of WSJ)\n",
      " - 2: Parser\n",
      " - 3: Training\n",
      " - 4: WSJ 23 F1\n",
      " - 5: Vinyals & Kaiser el al. (2014) [37]\n",
      " - 6: WSJ only, discriminative\n",
      " - 7: 88.3\n",
      " - 8: Petrov et al. (2006) [29]\n",
      " - 9: WSJ only, discriminative\n",
      " - 10: 90.4\n",
      " - 11: Zhu et al. (2013) [40]\n",
      " - 12: WSJ only, discriminative\n",
      " - 13: 90.4\n",
      " - 14: Dyer et al. (2016) [8]\n",
      " - 15: WSJ only, discriminative\n",
      " - 16: 91.7\n",
      " - 17: Transformer (4 layers)\n",
      " - 18: WSJ only, discriminative\n",
      " - 19: 91.3\n",
      " - 20: Zhu et al. (2013) [40]\n",
      " - 21: semi-supervised\n",
      " - 22: 91.3\n",
      " - 23: Huang & Harper (2009) [14]\n",
      " - 24: semi-supervised\n",
      " - 25: 91.3\n",
      " - 26: McClosky et al. (2006) [26]\n",
      " - 27: semi-supervised\n",
      " - 28: 92.1\n",
      " - 29: Vinyals & Kaiser el al. (2014) [37]\n",
      " - 30: semi-supervised\n",
      " - 31: 92.1\n",
      " - 32: Transformer (4 layers)\n",
      " - 33: semi-supervised\n",
      " - 34: 92.7\n",
      " - 35: Luong et al. (2015) [23]\n",
      " - 36: multi-task\n",
      " - 37: 93.0\n",
      " - 38: Dyer et al. (2016) [8]\n",
      " - 39: generative\n",
      " - 40: 93.3\n",
      " - 41: increased the maximum output length to input length + 300. We used a beam size of 21 and @ = 0.3\n",
      " - 42: for both WSJ only and the semi-supervised setting.\n",
      " - 43: Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      " - 44: prisingly well, yielding better results than all previously reported models with the exception of the\n",
      " - 45: Recurrent Neural Network Grammar [8].\n",
      " - 46: In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\n",
      " - 47: Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      " - 48: 7 Conclusion\n",
      " - 49: In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      " - 50: attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      " - 51: multi-headed self-attention.\n",
      " - 52: For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      " - 53: on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      " - 54: English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      " - 55: model outperforms even all previously reported ensembles.\n",
      " - 56: We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      " - 57: plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      " - 58: to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      " - 59: such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      " - 60: The code we used to train and evaluate our models is available at https://github.com/\n",
      " - 61: tensorflow/tensor2tensor.\n",
      " - 62: Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      " - 63: comments, corrections and inspiration.\n",
      " - 64: References\n",
      " - 65: [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      " - 66: arXiv: 1607.06450, 2016.\n",
      " - 67: [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      " - 68: learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      " - 69: [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\n",
      " - 70: machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      " - 71: [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      " - 72: reading. arXiv preprint arXiv: 1601.06733, 2016.\n",
      " - 73: 10\n",
      "Page #11 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      " - 1: and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      " - 2: machine translation. CoRR, abs/1406.1078, 2014.\n",
      " - 3: [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      " - 4: preprint arXiv: 1610.02357, 2016.\n",
      " - 5: [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      " - 6: of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      " - 7: [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      " - 8: network grammars. In Proc. of NAACL, 2016.\n",
      " - 9: [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      " - 10: tional sequence to sequence learning. arXiv preprint arXiv: 1705.03122v2, 2017.\n",
      " - 11: [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
      " - 12: arXiv: 1308.0850, 2013.\n",
      " - 13: [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      " - 14: age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      " - 15: Recognition, pages 770-778, 2016.\n",
      " - 16: [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      " - 17: recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      " - 18: [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
      " - 19: 9(8):1735-1780, 1997.\n",
      " - 20: [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      " - 21: across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      " - 22: Language Processing, pages 832-841. ACL, August 2009.\n",
      " - 23: [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      " - 24: the limits of language modeling. arXiv preprint arXiv: 1602.02410, 2016.\n",
      " - 25: [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      " - 26: Information Processing Systems, (NIPS), 2016.\n",
      " - 27: [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      " - 28: on Learning Representations (ICLR), 2016.\n",
      " - 29: [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      " - 30: ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv: 1610.10099v2,\n",
      " - 31: 2017.\n",
      " - 32: [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      " - 33: In International Conference on Learning Representations, 2017.\n",
      " - 34: [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      " - 35: [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      " - 36: arXiv: 1703.10722, 2017.\n",
      " - 37: [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      " - 38: Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      " - 39: arXiv: 1703.03130, 2017.\n",
      " - 40: [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      " - 41: sequence to sequence learning. arXiv preprint arXiv: 1511.06114, 2015.\n",
      " - 42: [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      " - 43: based neural machine translation. arXiv preprint arXiv: 1508.04025, 2015.\n",
      " - 44: 11\n",
      "Page #12 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      " - 1: corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.\n",
      " - 2: [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      " - 3: Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
      " - 4: pages 152-159. ACL, June 2006.\n",
      " - 5: [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      " - 6: model. In Empirical Methods in Natural Language Processing, 2016.\n",
      " - 7: [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      " - 8: summarization. arXiv preprint arXiv: 1705.04304, 2017.\n",
      " - 9: [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      " - 10: and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      " - 11: Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440. ACL, July\n",
      " - 12: 2006.\n",
      " - 13: [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      " - 14: preprint arXiv: 1608.05859, 2016.\n",
      " - 15: [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      " - 16: with subword units. arXiv preprint arXiv: 1508.07909, 2015.\n",
      " - 17: [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      " - 18: and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      " - 19: layer. arXiv preprint arXiv: 1701.06538, 2017.\n",
      " - 20: [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      " - 21: nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      " - 22: Learning Research, 15(1):1929-1958, 2014.\n",
      " - 23: [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      " - 24: networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      " - 25: Advances in Neural Information Processing Systems 28, pages 2440-2448. Curran Associates,\n",
      " - 26: Inc., 2015.\n",
      " - 27: [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      " - 28: networks. In Advances in Neural Information Processing Systems, pages 3104-3112, 2014.\n",
      " - 29: [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      " - 30: Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      " - 31: [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      " - 32: Advances in Neural Information Processing Systems, 2015.\n",
      " - 33: [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      " - 34: Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine\n",
      " - 35: translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      " - 36: arXiv: 1609.08144, 2016.\n",
      " - 37: [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      " - 38: fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      " - 39: [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      " - 40: shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      " - 41: 1: Long Papers), pages 434-443. ACL, August 2013.\n",
      " - 42: 12\n",
      "Page #13 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: Attention Visualizations\n",
      " - 1: the word 'making'. Different colors represent different heads. Best viewed in color.\n",
      " - 2: Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      " - 3: encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      " - 4: the verb 'making', completing the phrase 'making ... more difficult'. Attentions here shown only for\n",
      " - 5: It\n",
      " - 6: It\n",
      " - 7: is\n",
      " - 8: is\n",
      " - 9: in\n",
      " - 10: in\n",
      " - 11: this\n",
      " - 12: this\n",
      " - 13: spirit\n",
      " - 14: spirit\n",
      " - 15: that\n",
      " - 16: that\n",
      " - 17: a\n",
      " - 18: a\n",
      " - 19: majority\n",
      " - 20: majority\n",
      " - 21: of\n",
      " - 22: American\n",
      " - 23: of\n",
      " - 24: American\n",
      " - 25: governments\n",
      " - 26: governments\n",
      " - 27: have\n",
      " - 28: passed\n",
      " - 29: have\n",
      " - 30: passed\n",
      " - 31: new\n",
      " - 32: new\n",
      " - 33: laws\n",
      " - 34: laws\n",
      " - 35: since\n",
      " - 36: since\n",
      " - 37: 2009\n",
      " - 38: 2009\n",
      " - 39: making\n",
      " - 40: making\n",
      " - 41: the\n",
      " - 42: registration\n",
      " - 43: the\n",
      " - 44: registration\n",
      " - 45: or\n",
      " - 46: voting\n",
      " - 47: process\n",
      " - 48: or\n",
      " - 49: voting\n",
      " - 50: process\n",
      " - 51: more\n",
      " - 52: more\n",
      " - 53: difficult\n",
      " - 54: difficult\n",
      " - 55: .\n",
      " - 56: .\n",
      " - 57: <EOS>\n",
      " - 58: <EOS>\n",
      " - 59: <pad>\n",
      " - 60: <pad>\n",
      " - 61: <pad>\n",
      " - 62: <pad>\n",
      " - 63: <pad>\n",
      " - 64: <pad>\n",
      " - 65: <pad>\n",
      " - 66: <pad>\n",
      " - 67: <pad>\n",
      " - 68: <pad>\n",
      " - 69: <pad>\n",
      " - 70: <pad>\n",
      " - 71: 13\n",
      "Page #14 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: and 6. Note that the attentions are very sharp for this word.\n",
      " - 1: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5\n",
      " - 2: Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      " - 3: The\n",
      " - 4: The\n",
      " - 5: Law\n",
      " - 6: The\n",
      " - 7: The\n",
      " - 8: Law\n",
      " - 9: Law\n",
      " - 10: Law\n",
      " - 11: will\n",
      " - 12: will\n",
      " - 13: will\n",
      " - 14: will\n",
      " - 15: never\n",
      " - 16: be\n",
      " - 17: perfect\n",
      " - 18: never\n",
      " - 19: never\n",
      " - 20: never\n",
      " - 21: be\n",
      " - 22: be\n",
      " - 23: be\n",
      " - 24: perfect\n",
      " - 25: perfect\n",
      " - 26: perfect\n",
      " - 27: ,\n",
      " - 28: ,\n",
      " - 29: but\n",
      " - 30: ,\n",
      " - 31: but\n",
      " - 32: its\n",
      " - 33: but\n",
      " - 34: ,\n",
      " - 35: but\n",
      " - 36: its\n",
      " - 37: its\n",
      " - 38: its\n",
      " - 39: application\n",
      " - 40: application\n",
      " - 41: should\n",
      " - 42: application\n",
      " - 43: should\n",
      " - 44: application\n",
      " - 45: should\n",
      " - 46: should\n",
      " - 47: be\n",
      " - 48: be\n",
      " - 49: be\n",
      " - 50: be\n",
      " - 51: just\n",
      " - 52: just\n",
      " - 53: just\n",
      " - 54: just\n",
      " - 55: -\n",
      " - 56: -\n",
      " - 57: -\n",
      " - 58: -\n",
      " - 59: this\n",
      " - 60: this\n",
      " - 61: this\n",
      " - 62: this\n",
      " - 63: is\n",
      " - 64: is\n",
      " - 65: is\n",
      " - 66: is\n",
      " - 67: what\n",
      " - 68: what\n",
      " - 69: what\n",
      " - 70: what\n",
      " - 71: we\n",
      " - 72: we\n",
      " - 73: we\n",
      " - 74: we\n",
      " - 75: are\n",
      " - 76: are\n",
      " - 77: are\n",
      " - 78: are\n",
      " - 79: missing\n",
      " - 80: missing\n",
      " - 81: missing\n",
      " - 82: missing\n",
      " - 83: ,\n",
      " - 84: ,\n",
      " - 85: ,\n",
      " - 86: in\n",
      " - 87: my\n",
      " - 88: ,\n",
      " - 89: in\n",
      " - 90: in\n",
      " - 91: in\n",
      " - 92: my\n",
      " - 93: my\n",
      " - 94: my\n",
      " - 95: opinion\n",
      " - 96: opinion\n",
      " - 97: opinion\n",
      " - 98: opinion\n",
      " - 99: .\n",
      " - 100: .\n",
      " - 101: .\n",
      " - 102: .\n",
      " - 103: <EOS>\n",
      " - 104: <EOS>\n",
      " - 105: <EOS>\n",
      " - 106: <EOS>\n",
      " - 107: <pad>\n",
      " - 108: <pad>\n",
      " - 109: <pad>\n",
      " - 110: <pad>\n",
      " - 111: 14\n",
      "Page #15 has Width: 8.5 Height: 11 and measured with unit: LengthUnit.INCH\n",
      " - 0: The\n",
      " - 1: The\n",
      " - 2: Law\n",
      " - 3: Law\n",
      " - 4: will\n",
      " - 5: will\n",
      " - 6: never\n",
      " - 7: never\n",
      " - 8: be\n",
      " - 9: be\n",
      " - 10: perfect\n",
      " - 11: perfect\n",
      " - 12: ,\n",
      " - 13: ,\n",
      " - 14: but\n",
      " - 15: but\n",
      " - 16: its\n",
      " - 17: its\n",
      " - 18: application\n",
      " - 19: should\n",
      " - 20: application\n",
      " - 21: should\n",
      " - 22: be\n",
      " - 23: be\n",
      " - 24: just\n",
      " - 25: just\n",
      " - 26: -\n",
      " - 27: -\n",
      " - 28: this\n",
      " - 29: this\n",
      " - 30: is\n",
      " - 31: is\n",
      " - 32: what\n",
      " - 33: what\n",
      " - 34: we\n",
      " - 35: we\n",
      " - 36: are\n",
      " - 37: are\n",
      " - 38: missing\n",
      " - 39: missing\n",
      " - 40: ,\n",
      " - 41: in\n",
      " - 42: ,\n",
      " - 43: in\n",
      " - 44: my\n",
      " - 45: my\n",
      " - 46: opinion\n",
      " - 47: opinion\n",
      " - 48: .\n",
      " - 49: .\n",
      " - 50: <EOS>\n",
      " - 51: <EOS>\n",
      " - 52: <pad>\n",
      " - 53: <pad>\n",
      " - 54: The\n",
      " - 55: The\n",
      " - 56: Law\n",
      " - 57: Law\n",
      " - 58: will\n",
      " - 59: will\n",
      " - 60: never\n",
      " - 61: never\n",
      " - 62: be\n",
      " - 63: be\n",
      " - 64: perfect\n",
      " - 65: perfect\n",
      " - 66: ,\n",
      " - 67: but\n",
      " - 68: ,\n",
      " - 69: but\n",
      " - 70: its\n",
      " - 71: its\n",
      " - 72: application\n",
      " - 73: application\n",
      " - 74: should\n",
      " - 75: should\n",
      " - 76: be\n",
      " - 77: be\n",
      " - 78: just\n",
      " - 79: just\n",
      " - 80: -\n",
      " - 81: -\n",
      " - 82: this\n",
      " - 83: this\n",
      " - 84: is\n",
      " - 85: is\n",
      " - 86: what\n",
      " - 87: what\n",
      " - 88: we\n",
      " - 89: we\n",
      " - 90: are\n",
      " - 91: are\n",
      " - 92: missing\n",
      " - 93: missing\n",
      " - 94: ,\n",
      " - 95: ,\n",
      " - 96: in\n",
      " - 97: in\n",
      " - 98: my\n",
      " - 99: my\n",
      " - 100: opinion\n",
      " - 101: opinion\n",
      " - 102: .\n",
      " - 103: .\n",
      " - 104: <EOS>\n",
      " - 105: <EOS>\n",
      " - 106: <pad>\n",
      " - 107: <pad>\n",
      " - 108: Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      " - 109: sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      " - 110: at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      " - 111: 15\n"
     ]
    }
   ],
   "source": [
    "for page in result.pages:\n",
    "    print(f\"Page #{page.page_number} has Width: {page.width} Height: {page.height} and measured with unit: {page.unit}\")\n",
    "    if page.lines:\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            print(f\" - {line_idx}: {line.content}\")  # using line.polygon to get the placement of this content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81ac2f-ecba-4ae0-9c9a-afa99e765cdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Extracting tables in whole pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eab7af8-1624-4c3b-bf0f-603c6b2c5cbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table # 0 has 2 rows and 4 columns\n",
      "0                             1                           2                             3\n",
      "----------------------------  --------------------------  ----------------------------  --------------------------------\n",
      "Ashish Vaswani* Google Brain  Noam Shazeer* Google Brain  Niki Parmar* Google Research  Jakob Uszkoreit* Google Research\n",
      "avaswani@google.com           noam@google.com             nikip@google.com              usz@google.com\n",
      "Table # 1 has 2 rows and 3 columns\n",
      "0                             1                                        2\n",
      "----------------------------  ---------------------------------------  ---------------------------\n",
      "Llion Jones* Google Research  Aidan N. Gomez* + University of Toronto  Łukasz Kaiser* Google Brain\n",
      "llion@google.com              aidan@cs.toronto.edu                     lukaszkaiser@google.com\n",
      "Table # 2 has 5 rows and 4 columns\n",
      "0                            1                     2                      3\n",
      "---------------------------  --------------------  ---------------------  -------------------\n",
      "Layer Type                   Complexity per Layer  Sequential Operations  Maximum Path Length\n",
      "Self-Attention               O(n2 .d)              O(1)                   O(1)\n",
      "Recurrent                    O(n ·d2)              O(n)                   O(n)\n",
      "Convolutional                O(k ·n·d2)            O(1)                   O(logk(n))\n",
      "Self-Attention (restricted)  O(r . n . d)          O(1)                   O(n/r)\n",
      "                             :unselected:                                 :unselected:\n",
      "Table # 3 has 12 rows and 5 columns\n",
      "0                                1      2      3                      4\n",
      "-------------------------------  -----  -----  ---------------------  ----------\n",
      "Model                            BLEU          Training Cost (FLOPs)\n",
      "                                 EN-DE  EN-FR  EN-DE                  EN-FR\n",
      "ByteNet [18]                     23.75\n",
      "Deep-Att + PosUnk [39]                  39.2                          1.0 · 1020\n",
      "GNMT + RL [38]                   24.6   39.92  2.3 . 1019             1.4 . 1020\n",
      "ConvS2S [9]                      25.16  40.46  9.6 . 1018             1.5 . 1020\n",
      "MoE [32]                         26.03  40.56  2.0 · 1019             1.2 . 1020\n",
      "Deep-Att + PosUnk Ensemble [39]         40.4                          8.0 · 1020\n",
      "GNMT + RL Ensemble [38]          26.30  41.16  1.8 . 1020             1.1 . 1021\n",
      "ConvS2S Ensemble [9]             26.36  41.29  7.7 .1019              1.2 . 1021\n",
      "Transformer (base model)         27.3   38.1   3.3 .                  1018\n",
      "Transformer (big)                28.4   41.8   2.3 · 1019\n",
      "Table # 4 has 21 rows and 13 columns\n",
      "0     1                     2       3     4    5    6           7          8    9            10         11          12\n",
      "----  --------------------  ------  ----  ---  ---  ----------  ---------  ---  -----------  ---------  ----------  ------------\n",
      "      N                     dmodel  dff   h    dk   dv          Pdrop      Els  train steps  PPL (dev)  BLEU (dev)  params × 106\n",
      "base  6                     512     2048  8    64   64          0.1        0.1  100K         4.92       25.8        65\n",
      "(A)                                       1    512  512                                      5.29       24.9\n",
      "                                          4    128  128                                      5.00       25.5\n",
      "                                          16   32   32                                       4.91       25.8\n",
      "                                          32   16   16                                       5.01       25.4\n",
      "(B)                                            16                                            5.16       25.1        58\n",
      "                                               32                                            5.01       25.4        60\n",
      "(C)   2                                                                                      6.11       23.7        36\n",
      "      4                                                                                      5.19       25.3        50\n",
      "      8                                                                                      4.88       25.5        80\n",
      "                            256                32   32                                       5.75       24.5        28\n",
      "                            1024               128  128                                      4.66       26.0        168\n",
      "                                    1024                                                     5.12       25.4        53\n",
      "                                    4096                                                     4.75       26.2        90\n",
      "(D)                                                             0.0                          5.77       24.6\n",
      "                                                                0.2                          4.95       25.5\n",
      "                                                                           0.0               4.67       25.3\n",
      "                                                                           0.2               5.47       25.7\n",
      "(E)   positional embedding                          instead of  sinusoids                    4.92       25.7\n",
      "big   6                     1024    4096  16                    0.3             300K         4.33       26.4        213\n",
      "Table # 5 has 13 rows and 3 columns\n",
      "0                                    1                         2\n",
      "-----------------------------------  ------------------------  ---------\n",
      "Parser                               Training                  WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37]  WSJ only, discriminative  88.3\n",
      "Petrov et al. (2006) [29]            WSJ only, discriminative  90.4\n",
      "Zhu et al. (2013) [40]               WSJ only, discriminative  90.4\n",
      "Dyer et al. (2016) [8]               WSJ only, discriminative  91.7\n",
      "Transformer (4 layers)               WSJ only, discriminative  91.3\n",
      "Zhu et al. (2013) [40]               semi-supervised           91.3\n",
      "Huang & Harper (2009) [14]           semi-supervised           91.3\n",
      "McClosky et al. (2006) [26]          semi-supervised           92.1\n",
      "Vinyals & Kaiser el al. (2014) [37]  semi-supervised           92.1\n",
      "Transformer (4 layers)               semi-supervised           92.7\n",
      "Luong et al. (2015) [23]             multi-task                93.0\n",
      "Dyer et al. (2016) [8]               generative                93.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "if result.tables:\n",
    "    for table_idx, table in enumerate(result.tables):\n",
    "        print(f\"Table # {table_idx} has {table.row_count} rows and \" f\"{table.column_count} columns\")\n",
    "        output = [[None for _ in range(table.column_count)] for _ in range(table.row_count)]\n",
    "        for cell in table.cells:\n",
    "            output[cell.row_index][cell.column_index] = cell.content\n",
    "        print(tabulate(output, headers=[str(i) for i in range(table.column_count)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5133c0c-cadc-48c1-818d-bdb21f695a3c",
   "metadata": {},
   "source": [
    "### Deploy your Cognitive Module in Local Container\n",
    "\n",
    "According to [Official Tutorial](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/containers/install-run?view=doc-intel-4.0.0&tabs=read), Dockernize Document Intelligence Service only support for following modules.\n",
    "\n",
    "* **Read, Layout, ID Document, Receipt,** and **Invoice** modules are supported by Document Intelligence v3.1 containers.\n",
    "* **Read, Layout, General Document, Business Card,** and **Custom** modules are supported by Document Intelligence v3.0 containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda4368-d078-4674-a374-0b95440d128b",
   "metadata": {},
   "source": [
    "### **Prerequisites**\n",
    "* **Docker Engine** installed\n",
    "* Enough **CPU Cores** and **Memory** for given service\n",
    "\n",
    "    <img src=\"./assets/system-requirements-for-documents-intelligence.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa49937-8acd-4770-8cf3-56a1f1a7e98b",
   "metadata": {},
   "source": [
    "#### Step1. Fill the Configuration of your Module and Generate `docker-compose.yml` File<sub>[[More YML Details]](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/containers/install-run?view=doc-intel-4.0.0&tabs=read)\n",
    "\n",
    "* `ApiKey`: The value of this option must be set to a key for the provisioned resource specified in Billing.\n",
    "* `Billing`: The value of this option must be set to the endpoint URI of a provisioned Azure resource.\n",
    "* `Eula`: Indicates that you accepted the license for the container. The value of this option must be set to accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e2c42b-b70a-406b-a457-04c754d39296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "IMAGE_URL = 'mcr.microsoft.com/azure-cognitive-services/form-recognizer/layout-3.1'\n",
    "CONTAINER_NAME = 'azure-form-recognizer-layout'\n",
    "docker_compose_config = {\n",
    "    'version': '3.9',\n",
    "    'services':{\n",
    "        'azure-form-recognizer-read':{\n",
    "            'container_name': CONTAINER_NAME,\n",
    "            'image': IMAGE_URL,\n",
    "            'environment':[\n",
    "                'EULA=accept', \n",
    "                f'billing={FORM_RECOGNIZER_ENDPOINT_URI}', \n",
    "                f'apiKey={FORM_RECOGNIZER_KEY}'],\n",
    "            'ports':['5000:5000'],\n",
    "            'networks':['ocrvnet']\n",
    "        }\n",
    "    },\n",
    "    'networks':{\n",
    "        'ocrvnet':{\n",
    "            'driver': 'bridge'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "with open('docker-compose.yml', 'w') as f:\n",
    "    yaml.dump(docker_compose_config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f978ea4-cee6-4fd3-8d8a-d94deb5d0f9d",
   "metadata": {},
   "source": [
    "#### Step2. Start the service with the docker compose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09ca57-bf77-4911-9d1d-b144dad79365",
   "metadata": {},
   "source": [
    "run `docker-compose up` in your Terminal or Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c6085b-1dee-47c4-98f9-e3797390028f",
   "metadata": {},
   "source": [
    "#### Step3. Validate that the service is running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a6d180-6c28-4569-89a3-3b859ccf291e",
   "metadata": {},
   "source": [
    "a. Open a new browser tab and use the Endpoint RUL `http://localhost:5000`<br>\n",
    "b. Select **Service API Description** to view the swagger page, and select any of the POST APIs and select Try it out<br><br>\n",
    "    <img src=\"./assets/navigator-for-container.png\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502966c6-9243-4457-b38d-98ae2876fff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"service\":\"formrecognizerlayout\",\"apiStatus\":\"Valid\",\"apiStatusMessage\":\"Api Key is valid, no action needed.\"}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://localhost:5000/status')\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12967835-ddc8-4945-9f2c-cd4709f139f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"service\":\"formrecognizerlayout\",\"ready\":\"ready\",\"message\":\"Api Key is valid, no action needed.\"}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('http://localhost:5000/ready')\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e9113-bbcd-486b-a609-50c3a5be3013",
   "metadata": {},
   "source": [
    "**【Note】** To use this container in a disconnected environment, please submit a **REQUEST FORM** and **PURCHASE A COMMITMENT PLAN** to Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7b8f1-1de6-4b6f-91ed-aa9dd76b3725",
   "metadata": {},
   "source": [
    "### Appendix -  An Overview of Containers Supporting Disconnected Environments<sub> (2024/10 updated)\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-container-support?view=doc-intel-3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4785f5-2bcb-4321-8bc7-04506421e017",
   "metadata": {},
   "source": [
    "#### Language containers\n",
    "|  Service   | Description  | Availability  |\n",
    "|  --------  | -----------  | ------------- |\n",
    "| Key Phrase Extraction | Extracts key phrases to identify the main points. | Generally Available |\n",
    "| Text Language Detection | For up to 120 languages, detects which language the input text is written in and report a single language code for every document submitted on the request.  |  Generally Available|\n",
    "| Sentiment Analysis | Analyzes raw text for clues about positive or negative sentiment. This version of sentiment analysis returns sentiment labels | Generally Available |\n",
    "| Named Entity Recognition | Extract named entities from text. | Generally Available |\n",
    "| Summarization | Summarize text from various sources.\t | Generally Available |\n",
    "| ranslator | Translate text in several languages and dialects. | Generally Available |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7bd1ad-9a7f-4984-a3c6-a75742491035",
   "metadata": {},
   "source": [
    "#### Speech containers\n",
    "|  Service   | Description  | Status  |\n",
    "|  --------  | -----------  | ------------- |\n",
    "| Speech to text  | Transcribes continuous real-time speech into text.| Generally Available|\n",
    "| Custom Speech to text  | Transcribes continuous real-time speech into text using a custom model. | Generally Available |\n",
    "| Neural Text to speech  | Converts text to natural-sounding speech using deep neural network technology, allowing for more natural synthesized speech. | Generally Available|\n",
    "| Speech language identification | Determines the language of spoken audio. | Preview |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba6a82-6f89-4358-80e5-b5e86aa8b542",
   "metadata": {},
   "source": [
    "#### Vision containers\n",
    "|  Service   | Description  | Status  |\n",
    "|  --------  | -----------  | ------------- |\n",
    "| Read OCR  | Extract printed and handwritten text from images and documents, support for `JPEG`, `PNG`, `BMP`, `PDF`, and `TIFF` file formats.| Generally Available |\n",
    "| Spatial analysis | Analyzes real-time streaming video to understand spatial relationships between people, their movement, and interactions with objects in physical. | Preview |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
